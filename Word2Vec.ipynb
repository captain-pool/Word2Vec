{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Word2Vec.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "w71OgdV2YtT1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3783
        },
        "outputId": "9d0f8e03-0e64-4d16-f577-6f0672a895e5"
      },
      "cell_type": "code",
      "source": [
        "!wget https://github.com/Phylliida/Dialogue-Datasets/raw/master/MovieCorpus.txt\n",
        "!python -m nltk.downloader all"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2018-10-11 07:28:42--  https://github.com/Phylliida/Dialogue-Datasets/raw/master/MovieCorpus.txt\n",
            "Resolving github.com (github.com)... 192.30.253.112, 192.30.253.113\n",
            "Connecting to github.com (github.com)|192.30.253.112|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/Phylliida/Dialogue-Datasets/master/MovieCorpus.txt [following]\n",
            "--2018-10-11 07:28:43--  https://raw.githubusercontent.com/Phylliida/Dialogue-Datasets/master/MovieCorpus.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 16976724 (16M) [text/plain]\n",
            "Saving to: ‘MovieCorpus.txt’\n",
            "\n",
            "MovieCorpus.txt     100%[===================>]  16.19M  72.8MB/s    in 0.2s    \n",
            "\n",
            "2018-10-11 07:28:46 (72.8 MB/s) - ‘MovieCorpus.txt’ saved [16976724/16976724]\n",
            "\n",
            "/usr/lib/python3.6/runpy.py:125: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
            "  warn(RuntimeWarning(msg))\n",
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/omw.zip.\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
            "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "jzLpjOx_Dh-k",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import string\n",
        "from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n",
        "import numpy as np\n",
        "from nltk.corpus import stopwords\n",
        "import warnings\n",
        "import tqdm\n",
        "import os\n",
        "import tensorflow as tf\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jfY5sdNYDPsJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print_interval = 1000\n",
        "window_size = 5\n",
        "n_epoch = int(1e4)\n",
        "batch_size = 32"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ACuN81ae_vkl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "787cc40c-1de8-43c5-986c-7b62d4a40b64"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "sw = stopwords.words(\"english\")\n",
        "print(sw)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_e9ImmzZ1zs2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "with open(\"MovieCorpus.txt\",\"r\") as f:\n",
        "  punc_dict = str.maketrans('','',string.punctuation)\n",
        "  l = f.read().split('\\n')\n",
        "  l = [word.translate(punc_dict) for word in l if not word in sw]\n",
        "  l = [word.lower() for word in l if word.isalpha()]\n",
        "  l = sorted(list(set(l)),key=l.index)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JLU3YzyzC72X",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "le = LabelEncoder().fit(l)\n",
        "oh = OneHotEncoder().fit(le.transform(l).reshape(len(l),1))\n",
        "def transform(w):\n",
        "  return oh.transform(le.transform([w]).reshape(1,1))\n",
        "def inv_transform(w):\n",
        "  return  le.inverse_transform(np.argmax(w))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "w4aGGQzjL0Fk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4f1bcf85-1ba6-48d1-e832-c0aed841687f"
      },
      "cell_type": "code",
      "source": [
        "def vectorize(l):\n",
        "  X = []\n",
        "  temp = []\n",
        "  w = 0\n",
        "  pbar = tqdm.tqdm(total = (len(l)-window_size))\n",
        "  while w < len(l):\n",
        "    if len(temp)<window_size:\n",
        "      temp.append(transform(l[w]))\n",
        "      w+=1\n",
        "    else:\n",
        "      X.append(temp)\n",
        "      w = w - (window_size - 1)\n",
        "      temp = []\n",
        "      pbar.update()\n",
        "  if len(l) == window_size:\n",
        "    X.append(temp)\n",
        "    temp = []\n",
        "  pbar.close()\n",
        "  return X\n",
        "X = vectorize(l)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 4117/4117 [00:52<00:00, 66.21it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "cFoHCEvzNZ60",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dict_len = len(l)\n",
        "X = np.array(X)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Q_MiVn7SFyvo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "9uRPtgTzXlmW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "def gen_weights(shape):\n",
        "  return tf.Variable(tf.random_normal(shape))\n",
        "def gen_bias(shape):\n",
        "  return tf.Variable(tf.random_normal([shape]))\n",
        "def flatten(in_):\n",
        "  return tf.reshape(in_,[int(in_.shape[0]),in_.shape[1:].num_elements()])\n",
        "def dense(in_,out):\n",
        "  return tf.matmul(in_,gen_weights([int(in_.shape[-1]),out]))+gen_bias(out)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ePUIfRjaRIYx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def convert_func(m):\n",
        "  ret = []\n",
        "  ret2 = []\n",
        "  for i in m:\n",
        "    for j in i: #len(i) == window_size\n",
        "      ret.append(j.toarray().flatten())\n",
        "    ret2.append(i[len(i)>>1].toarray().flatten())\n",
        "  ret = np.array(ret).reshape([len(m),window_size,dict_len])\n",
        "  ret2 = np.array(ret2).reshape([len(m),dict_len])\n",
        "  return ret,ret2\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GBkmwslpX-ek",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x = tf.placeholder(tf.float32,[None,window_size*dict_len])\n",
        "y = tf.placeholder(tf.float32,[None,dict_len])\n",
        "def encoder(X,out_l):\n",
        "  h = dense(X,1024)\n",
        " # h = dense(h,128)\n",
        "  out = dense(h,out_l)\n",
        "  #h = dense(out,128)\n",
        "  h = dense(h,1024)\n",
        "  fin = dense(h,dict_len)\n",
        "  return fin,out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WMoi0pflOx72",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "339b21d2-6347-4428-a3a0-dddef814340d"
      },
      "cell_type": "code",
      "source": [
        "model,vec = encoder(x,128)\n",
        "print(model.shape,vec.shape)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(?, 4122) (?, 128)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "uo_zm-DYPOsA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels = y,logits = model))\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1eCilDyQQije",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "outputId": "be21babe-9010-43d3-84f7-f3ca1c4e5c6d"
      },
      "cell_type": "code",
      "source": [
        "sess = tf.Session()\n",
        "with tf.device('/gpu:0'):\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "  for i in range(n_epoch):\n",
        "      items = np.random.randint(0,len(X)-window_size,batch_size)\n",
        "      x_input,y_input = convert_func(X[items])\n",
        "      _,l = sess.run([optimizer,loss], feed_dict={x:x_input.reshape(batch_size,x_input.shape[1]*x_input.shape[2]),y:y_input.reshape(batch_size,y_input.shape[1])})\n",
        "      if not i%print_interval:\n",
        "        print(\"EPOCH:%d Loss:%f\"%(i,l))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "EPOCH:0 Loss:8894.072266\n",
            "EPOCH:1000 Loss:5908.673828\n",
            "EPOCH:2000 Loss:5221.234375\n",
            "EPOCH:3000 Loss:3428.783691\n",
            "EPOCH:4000 Loss:1589.008789\n",
            "EPOCH:5000 Loss:630.587402\n",
            "EPOCH:6000 Loss:205.105042\n",
            "EPOCH:7000 Loss:0.912004\n",
            "EPOCH:8000 Loss:49.700424\n",
            "EPOCH:9000 Loss:0.000000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "LYqNoOyjT8KV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2150
        },
        "outputId": "6ec90318-d06d-4dbf-9267-035e0a2eff73"
      },
      "cell_type": "code",
      "source": [
        "vec_dict = {}\n",
        "def create_vec_dict():\n",
        "  for i in tqdm.tqdm(range(len(X))):\n",
        "    x_input,y_input = convert_func(np.array([X[i]]))\n",
        "    vec_dict[inv_transform(y_input[0])] = sess.run(vec,feed_dict={x:x_input.reshape(1,x_input.shape[1]*x_input.shape[2])}).flatten()\n",
        "create_vec_dict()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/4117 [00:00<?, ?it/s]\u001b[A\n",
            "  1%|          | 31/4117 [00:00<00:13, 307.05it/s]\u001b[A\n",
            "  2%|▏         | 64/4117 [00:00<00:12, 313.06it/s]\u001b[A\n",
            "  2%|▏         | 98/4117 [00:00<00:12, 318.46it/s]\u001b[A\n",
            "  3%|▎         | 132/4117 [00:00<00:12, 323.84it/s]\u001b[A\n",
            "  4%|▍         | 166/4117 [00:00<00:12, 326.11it/s]\u001b[A\n",
            "  5%|▍         | 200/4117 [00:00<00:11, 328.39it/s]\u001b[A\n",
            "  6%|▌         | 234/4117 [00:00<00:11, 328.40it/s]\u001b[A\n",
            "  6%|▋         | 265/4117 [00:00<00:12, 319.11it/s]\u001b[A\n",
            "  7%|▋         | 299/4117 [00:00<00:11, 325.08it/s]\u001b[A\n",
            "  8%|▊         | 333/4117 [00:01<00:11, 327.20it/s]\u001b[A\n",
            "  9%|▉         | 367/4117 [00:01<00:11, 329.65it/s]\u001b[A\n",
            " 10%|▉         | 400/4117 [00:01<00:11, 329.13it/s]\u001b[A\n",
            " 11%|█         | 434/4117 [00:01<00:11, 331.26it/s]\u001b[A\n",
            " 11%|█▏        | 468/4117 [00:01<00:11, 331.66it/s]\u001b[A\n",
            " 12%|█▏        | 502/4117 [00:01<00:10, 332.48it/s]\u001b[A\n",
            " 13%|█▎        | 537/4117 [00:01<00:10, 335.36it/s]\u001b[A\n",
            " 14%|█▍        | 571/4117 [00:01<00:10, 334.89it/s]\u001b[A\n",
            " 15%|█▍        | 605/4117 [00:01<00:10, 335.28it/s]\u001b[A\n",
            " 16%|█▌        | 639/4117 [00:01<00:10, 333.97it/s]\u001b[A\n",
            " 16%|█▋        | 673/4117 [00:02<00:10, 327.32it/s]\u001b[A\n",
            " 17%|█▋        | 706/4117 [00:02<00:10, 320.94it/s]\u001b[A\n",
            " 18%|█▊        | 739/4117 [00:02<00:10, 321.80it/s]\u001b[A\n",
            " 19%|█▉        | 773/4117 [00:02<00:10, 324.60it/s]\u001b[A\n",
            " 20%|█▉        | 806/4117 [00:02<00:10, 322.62it/s]\u001b[A\n",
            " 20%|██        | 839/4117 [00:02<00:10, 323.65it/s]\u001b[A\n",
            " 21%|██        | 872/4117 [00:02<00:10, 321.08it/s]\u001b[A\n",
            " 22%|██▏       | 905/4117 [00:02<00:10, 318.60it/s]\u001b[A\n",
            " 23%|██▎       | 937/4117 [00:02<00:10, 317.09it/s]\u001b[A\n",
            " 24%|██▎       | 970/4117 [00:02<00:09, 319.85it/s]\u001b[A\n",
            " 24%|██▍       | 1002/4117 [00:03<00:09, 315.75it/s]\u001b[A\n",
            " 25%|██▌       | 1035/4117 [00:03<00:09, 318.21it/s]\u001b[A\n",
            " 26%|██▌       | 1067/4117 [00:03<00:09, 312.40it/s]\u001b[A\n",
            " 27%|██▋       | 1099/4117 [00:03<00:09, 305.90it/s]\u001b[A\n",
            " 27%|██▋       | 1131/4117 [00:03<00:09, 308.29it/s]\u001b[A\n",
            " 28%|██▊       | 1165/4117 [00:03<00:09, 315.42it/s]\u001b[A\n",
            " 29%|██▉       | 1198/4117 [00:03<00:09, 317.14it/s]\u001b[A\n",
            " 30%|██▉       | 1231/4117 [00:03<00:09, 319.93it/s]\u001b[A\n",
            " 31%|███       | 1264/4117 [00:03<00:08, 319.64it/s]\u001b[A\n",
            " 32%|███▏      | 1297/4117 [00:04<00:08, 320.95it/s]\u001b[A\n",
            " 32%|███▏      | 1330/4117 [00:04<00:08, 323.49it/s]\u001b[A\n",
            " 33%|███▎      | 1363/4117 [00:04<00:08, 321.29it/s]\u001b[A\n",
            " 34%|███▍      | 1396/4117 [00:04<00:08, 320.88it/s]\u001b[A\n",
            " 35%|███▍      | 1429/4117 [00:04<00:08, 322.16it/s]\u001b[A\n",
            " 36%|███▌      | 1462/4117 [00:04<00:08, 319.88it/s]\u001b[A\n",
            " 36%|███▋      | 1494/4117 [00:04<00:08, 307.26it/s]\u001b[A\n",
            " 37%|███▋      | 1527/4117 [00:04<00:08, 312.90it/s]\u001b[A\n",
            " 38%|███▊      | 1560/4117 [00:04<00:08, 316.13it/s]\u001b[A\n",
            " 39%|███▊      | 1593/4117 [00:04<00:07, 318.56it/s]\u001b[A\n",
            " 39%|███▉      | 1626/4117 [00:05<00:07, 321.89it/s]\u001b[A\n",
            " 40%|████      | 1659/4117 [00:05<00:07, 319.23it/s]\u001b[A\n",
            " 41%|████      | 1691/4117 [00:05<00:07, 319.41it/s]\u001b[A\n",
            " 42%|████▏     | 1724/4117 [00:05<00:07, 320.62it/s]\u001b[A\n",
            " 43%|████▎     | 1757/4117 [00:05<00:07, 323.26it/s]\u001b[A\n",
            " 43%|████▎     | 1790/4117 [00:05<00:07, 323.59it/s]\u001b[A\n",
            " 44%|████▍     | 1823/4117 [00:05<00:07, 321.83it/s]\u001b[A\n",
            " 45%|████▌     | 1856/4117 [00:05<00:07, 321.85it/s]\u001b[A\n",
            " 46%|████▌     | 1889/4117 [00:05<00:07, 312.11it/s]\u001b[A\n",
            " 47%|████▋     | 1922/4117 [00:05<00:06, 315.01it/s]\u001b[A\n",
            " 47%|████▋     | 1955/4117 [00:06<00:06, 318.14it/s]\u001b[A\n",
            " 48%|████▊     | 1988/4117 [00:06<00:06, 320.01it/s]\u001b[A\n",
            " 49%|████▉     | 2021/4117 [00:06<00:06, 320.06it/s]\u001b[A\n",
            " 50%|████▉     | 2054/4117 [00:06<00:06, 319.45it/s]\u001b[A\n",
            " 51%|█████     | 2086/4117 [00:06<00:06, 317.08it/s]\u001b[A\n",
            " 51%|█████▏    | 2119/4117 [00:06<00:06, 318.76it/s]\u001b[A\n",
            " 52%|█████▏    | 2152/4117 [00:06<00:06, 320.63it/s]\u001b[A\n",
            " 53%|█████▎    | 2186/4117 [00:06<00:05, 324.74it/s]\u001b[A\n",
            " 54%|█████▍    | 2219/4117 [00:06<00:05, 324.89it/s]\u001b[A\n",
            " 55%|█████▍    | 2252/4117 [00:06<00:05, 326.25it/s]\u001b[A\n",
            " 56%|█████▌    | 2285/4117 [00:07<00:05, 312.85it/s]\u001b[A\n",
            " 56%|█████▋    | 2317/4117 [00:07<00:05, 314.01it/s]\u001b[A\n",
            " 57%|█████▋    | 2350/4117 [00:07<00:05, 316.68it/s]\u001b[A\n",
            " 58%|█████▊    | 2383/4117 [00:07<00:05, 318.89it/s]\u001b[A\n",
            " 59%|█████▊    | 2416/4117 [00:07<00:05, 321.96it/s]\u001b[A\n",
            " 59%|█████▉    | 2449/4117 [00:07<00:05, 322.84it/s]\u001b[A\n",
            " 60%|██████    | 2482/4117 [00:07<00:05, 323.19it/s]\u001b[A\n",
            " 61%|██████    | 2515/4117 [00:07<00:04, 324.06it/s]\u001b[A\n",
            " 62%|██████▏   | 2548/4117 [00:07<00:04, 325.39it/s]\u001b[A\n",
            " 63%|██████▎   | 2581/4117 [00:08<00:04, 325.26it/s]\u001b[A\n",
            " 63%|██████▎   | 2614/4117 [00:08<00:04, 322.86it/s]\u001b[A\n",
            " 64%|██████▍   | 2647/4117 [00:08<00:04, 319.42it/s]\u001b[A\n",
            " 65%|██████▌   | 2679/4117 [00:08<00:04, 308.31it/s]\u001b[A\n",
            " 66%|██████▌   | 2711/4117 [00:08<00:04, 310.13it/s]\u001b[A\n",
            " 67%|██████▋   | 2745/4117 [00:08<00:04, 316.41it/s]\u001b[A\n",
            " 67%|██████▋   | 2777/4117 [00:08<00:04, 316.31it/s]\u001b[A\n",
            " 68%|██████▊   | 2810/4117 [00:08<00:04, 317.20it/s]\u001b[A\n",
            " 69%|██████▉   | 2844/4117 [00:08<00:03, 321.29it/s]\u001b[A\n",
            " 70%|██████▉   | 2877/4117 [00:08<00:03, 322.92it/s]\u001b[A\n",
            " 71%|███████   | 2910/4117 [00:09<00:03, 321.41it/s]\u001b[A\n",
            " 71%|███████▏  | 2943/4117 [00:09<00:03, 323.42it/s]\u001b[A\n",
            " 72%|███████▏  | 2976/4117 [00:09<00:03, 321.42it/s]\u001b[A\n",
            " 73%|███████▎  | 3009/4117 [00:09<00:03, 323.84it/s]\u001b[A\n",
            " 74%|███████▍  | 3042/4117 [00:09<00:03, 321.56it/s]\u001b[A\n",
            " 75%|███████▍  | 3075/4117 [00:09<00:03, 310.95it/s]\u001b[A\n",
            " 75%|███████▌  | 3108/4117 [00:09<00:03, 315.86it/s]\u001b[A\n",
            " 76%|███████▋  | 3140/4117 [00:09<00:03, 316.17it/s]\u001b[A\n",
            " 77%|███████▋  | 3172/4117 [00:09<00:03, 314.68it/s]\u001b[A\n",
            " 78%|███████▊  | 3204/4117 [00:09<00:02, 316.21it/s]\u001b[A\n",
            " 79%|███████▊  | 3237/4117 [00:10<00:02, 318.52it/s]\u001b[A\n",
            " 79%|███████▉  | 3269/4117 [00:10<00:02, 317.99it/s]\u001b[A\n",
            " 80%|████████  | 3301/4117 [00:10<00:02, 315.84it/s]\u001b[A\n",
            " 81%|████████  | 3334/4117 [00:10<00:02, 317.91it/s]\u001b[A\n",
            " 82%|████████▏ | 3366/4117 [00:10<00:02, 317.20it/s]\u001b[A\n",
            " 83%|████████▎ | 3398/4117 [00:10<00:02, 317.50it/s]\u001b[A\n",
            " 83%|████████▎ | 3431/4117 [00:10<00:02, 318.96it/s]\u001b[A\n",
            " 84%|████████▍ | 3463/4117 [00:10<00:02, 307.42it/s]\u001b[A\n",
            " 85%|████████▍ | 3495/4117 [00:10<00:02, 310.20it/s]\u001b[A\n",
            " 86%|████████▌ | 3527/4117 [00:11<00:01, 313.06it/s]\u001b[A\n",
            " 86%|████████▋ | 3560/4117 [00:11<00:01, 317.18it/s]\u001b[A\n",
            " 87%|████████▋ | 3594/4117 [00:11<00:01, 320.84it/s]\u001b[A\n",
            " 88%|████████▊ | 3627/4117 [00:11<00:01, 319.96it/s]\u001b[A\n",
            " 89%|████████▉ | 3660/4117 [00:11<00:01, 320.29it/s]\u001b[A\n",
            " 90%|████████▉ | 3693/4117 [00:11<00:01, 316.43it/s]\u001b[A\n",
            " 91%|█████████ | 3727/4117 [00:11<00:01, 320.90it/s]\u001b[A\n",
            " 91%|█████████▏| 3760/4117 [00:11<00:01, 322.34it/s]\u001b[A\n",
            " 92%|█████████▏| 3793/4117 [00:11<00:01, 323.03it/s]\u001b[A\n",
            " 93%|█████████▎| 3826/4117 [00:11<00:00, 322.96it/s]\u001b[A\n",
            " 94%|█████████▎| 3859/4117 [00:12<00:00, 313.11it/s]\u001b[A\n",
            " 95%|█████████▍| 3891/4117 [00:12<00:00, 311.77it/s]\u001b[A\n",
            " 95%|█████████▌| 3924/4117 [00:12<00:00, 316.95it/s]\u001b[A\n",
            " 96%|█████████▌| 3957/4117 [00:12<00:00, 318.38it/s]\u001b[A\n",
            " 97%|█████████▋| 3990/4117 [00:12<00:00, 320.63it/s]\u001b[A\n",
            " 98%|█████████▊| 4023/4117 [00:12<00:00, 315.78it/s]\u001b[A\n",
            " 99%|█████████▊| 4056/4117 [00:12<00:00, 318.00it/s]\u001b[A\n",
            " 99%|█████████▉| 4090/4117 [00:12<00:00, 322.28it/s]\u001b[A\n",
            "100%|██████████| 4117/4117 [00:12<00:00, 320.35it/s]\u001b[A"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "7-KRbAmeGP4O",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5v5ag4hMCajS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "with open(\"file-128.pkl\",\"wb\") as f:\n",
        "  pickle.dump(vec_dict,f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sasBZas1HOeA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"file-128.pkl\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rx6AR9AkWtmN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}